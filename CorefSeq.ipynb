{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_pretrained_bert\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 3.9MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (2.21.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (4.31.1)\r\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.0.1.post2)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.9.134)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (2019.4.14)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch_pretrained_bert) (1.16.3)\r\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (1.22)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (2.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch_pretrained_bert) (2019.3.9)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch_pretrained_bert) (0.2.0)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\r\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.134 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch_pretrained_bert) (1.12.134)\r\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.134->boto3->pytorch_pretrained_bert) (0.14)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.134->boto3->pytorch_pretrained_bert) (2.6.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.134->boto3->pytorch_pretrained_bert) (1.12.0)\r\n",
      "Installing collected packages: pytorch-pretrained-bert\r\n",
      "Successfully installed pytorch-pretrained-bert-0.6.2\r\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_stage_2.tsv', 'test_stage_1.tsv', 'sample_submission_stage_1.csv', 'sample_submission_stage_2.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from bson import ObjectId\n",
    "\n",
    "import pdb\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from io import open\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset, Dataset)\n",
    "\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from pytorch_pretrained_bert.modeling import BertForQuestionAnswering, BertForMultipleChoice, BertForPreTraining, BertConfig, BertModel, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.tokenization import (BasicTokenizer,\n",
    "                                                  BertTokenizer,\n",
    "                                                  whitespace_tokenize)\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-test.tsv\"\n",
    "dev_path = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\"\n",
    "val_path = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-validation.tsv\"\n",
    "\n",
    "dev_df = pd.read_csv(test_path, delimiter=\"\\t\")#pd.read_csv(dev_path, delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(dev_path, delimiter=\"\\t\")#pd.read_csv(test_path, delimiter=\"\\t\")\n",
    "val_df = pd.read_csv(val_path, delimiter=\"\\t\")\n",
    "\n",
    "#test_df_prod = test_df.copy()\n",
    "dev_df = pd.concat([dev_df, test_df])\n",
    "test_df_prod = pd.read_csv('../input/test_stage_2.tsv', delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_label(a_coref, b_coref):\n",
    "    if a_coref:\n",
    "        return 0\n",
    "    elif b_coref:\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "def get_gender(pronoun):\n",
    "    gender_mapping = {'he': 0, 'his': 0, 'him': 0, \n",
    "                      'she': 1, 'her': 1, 'hers': 1}\n",
    "    return gender_mapping.get(pronoun.lower(), 1)\n",
    "\n",
    "test_class_labels = [get_class_label(aco, bco) for aco, bco in zip(test_df['A-coref'], test_df['B-coref'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\r\n",
      "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\r\n",
      "\r\n",
      "## Package Plan ##\r\n",
      "\r\n",
      "  environment location: /opt/conda\r\n",
      "\r\n",
      "  removed specs:\r\n",
      "    - greenlet\r\n",
      "\r\n",
      "\r\n",
      "The following packages will be REMOVED:\r\n",
      "\r\n",
      "  gevent-1.3.0-py36h14c3975_0\r\n",
      "  greenlet-0.4.13-py36h14c3975_0\r\n",
      "\r\n",
      "\r\n",
      "Preparing transaction: | \b\bdone\r\n",
      "Verifying transaction: - \b\bdone\r\n",
      "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\r\n",
      "Collecting allennlp\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/c8/10342a6068a8d156a5947e03c95525d559e71ad62de0f2585ab922e14533/allennlp-0.8.3-py3-none-any.whl (5.6MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 5.6MB 6.1MB/s \r\n",
      "\u001b[?25hCollecting msgpack<0.6.0,>=0.5.6 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/4e/dcf124fd97e5f5611123d6ad9f40ffd6eb979d1efdc1049e28a795672fcd/msgpack-0.5.6-cp36-cp36m-manylinux1_x86_64.whl (315kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 317kB 22.5MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: unidecode in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.0.23)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.1.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.20.3)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2018.4)\r\n",
      "Collecting flaky (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/02/42/cca66659a786567c8af98587d66d75e7d2b6e65662f8daab75db708ac35b/flaky-3.5.3-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: pytest in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.5.1)\r\n",
      "Collecting parsimonious>=0.8.0 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 18.5MB/s \r\n",
      "\u001b[?25hCollecting word2number>=1.1 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\r\n",
      "Collecting moto>=1.3.4 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/35/f3dc85fa8fa2541b77849459363bb0d6e02bd44ba05afbd3ee3e035a6b14/moto-1.3.8-py2.py3-none-any.whl (580kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 583kB 19.7MB/s \r\n",
      "\u001b[?25hCollecting flask-cors>=3.0.7 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/65/cb/683f71ff8daa3aea0a5cbb276074de39f9ab66d3fbb8ad5efb5bb83e90d2/Flask_Cors-3.0.7-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: tqdm>=4.19 in /opt/conda/lib/python3.6/site-packages (from allennlp) (4.31.1)\r\n",
      "Collecting responses>=0.7 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/5a/b887e89925f1de7890ef298a74438371ed4ed29b33def9e6d02dc6036fd8/responses-0.10.6-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.6.2)\r\n",
      "Requirement already satisfied: flask>=1.0.2 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.0.2)\r\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.0.1.post2)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.9.134)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.9.0)\r\n",
      "Collecting conllu==0.11 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/2c/856344d9b69baf5b374c395b4286626181a80f0c2b2f704914d18a1cea47/conllu-0.11-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: spacy<2.2,>=2.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.1.3)\r\n",
      "Collecting awscli>=1.11.91 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/47/a18a1ea9df7c3354498005a1cf28ceb35b2977dfcc9e9f06f28fb7cfcb1c/awscli-1.16.152-py2.py3-none-any.whl (1.5MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 1.5MB 14.0MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: tensorboardX>=1.2 in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.6)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from allennlp) (1.16.3)\r\n",
      "Collecting jsonnet>=0.10.0; sys_platform != \"win32\" (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/dc/3abd3971869a741d7acdba166d71d4f9366b6b53028dfd56f95de356af0f/jsonnet-0.12.1.tar.gz (240kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 245kB 24.8MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: numpydoc>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from allennlp) (0.8.0)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.2.4)\r\n",
      "Collecting sqlparse>=0.2.4 (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/53/900f7d2a54557c6a37886585a91336520e5539e3ae2423ff1102daf4f3a7/sqlparse-0.3.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: requests>=2.18 in /opt/conda/lib/python3.6/site-packages (from allennlp) (2.21.0)\r\n",
      "Collecting gevent>=1.3.6 (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/ca/5b5962361ed832847b6b2f9a2d0452c8c2f29a93baef850bb8ad067c7bf9/gevent-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 5.5MB 7.8MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /opt/conda/lib/python3.6/site-packages (from allennlp) (3.0.3)\r\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.6/site-packages (from allennlp) (4.4.3)\r\n",
      "Collecting overrides (from allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\r\n",
      "Collecting editdistance (from allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/67/2b1fe72bdd13ee9ec32b97959d7dfbfcd7c0548081d69aaf8493c1e695f9/editdistance-0.5.3-cp36-cp36m-manylinux1_x86_64.whl (178kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 184kB 25.3MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: py>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (1.5.3)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (1.12.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (39.1.0)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (18.1.0)\r\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (4.1.0)\r\n",
      "Requirement already satisfied: pluggy<0.7,>=0.5 in /opt/conda/lib/python3.6/site-packages (from pytest->allennlp) (0.6.0)\r\n",
      "Requirement already satisfied: mock in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (2.0.0)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (2.6)\r\n",
      "Requirement already satisfied: botocore>=1.12.86 in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (1.12.134)\r\n",
      "Collecting python-jose<4.0.0 (from moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/96/da/c0dcc5e7a98a53440b8db3cf9771345fa696754f79e8734ea59123f7d734/python_jose-3.0.1-py2.py3-none-any.whl\r\n",
      "Collecting cryptography>=2.3.0 (from moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/12/b0409a94dad366d98a8eee2a77678c7a73aafd8c0e4b835abea634ea3896/cryptography-2.6.1-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 2.3MB 12.9MB/s \r\n",
      "\u001b[?25hCollecting xmltodict (from moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: werkzeug in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (0.14.1)\r\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (3.12)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (2.6.0)\r\n",
      "Collecting docker>=2.5.1 (from moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/68/c3afca1a5aa8d2997ec3b8ee822a4d752cf85907b321f07ea86888545152/docker-3.7.2-py2.py3-none-any.whl (134kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 25.6MB/s \r\n",
      "\u001b[?25hCollecting jsondiff==1.1.2 (from moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/33/0c/ddb17571e061c655871ccbf76cdada55a31569327d21517de779d4887241/jsondiff-1.1.2.tar.gz\r\n",
      "Requirement already satisfied: boto>=2.36.0 in /opt/conda/lib/python3.6/site-packages (from moto>=1.3.4->allennlp) (2.48.0)\r\n",
      "Collecting cfn-lint (from moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/82/f3c539fb6db5776b49138a8d9212a99313172d701b11e05824d224590483/cfn_lint-0.19.1-py3-none-any.whl (2.1MB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 2.1MB 14.2MB/s \r\n",
      "\u001b[?25hCollecting Jinja2>=2.10.1 (from moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/e7/fd8b501e7a6dfe492a433deb7b9d833d39ca74916fa8bc63dd1a4947a671/Jinja2-2.10.1-py2.py3-none-any.whl (124kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 25.8MB/s \r\n",
      "\u001b[?25hCollecting aws-xray-sdk!=0.96,>=0.93 (from moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/f2/79f7918f4ddeec525742ddd4607abe4a82a29a6bc4c7e297995f59a18965/aws_xray_sdk-2.4.2-py2.py3-none-any.whl (87kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 24.3MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.4.14)\r\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp) (0.24)\r\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/lib/python3.6/site-packages (from flask>=1.0.2->allennlp) (7.0)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->allennlp) (0.2.0)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->allennlp) (0.9.4)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (0.2.1)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (2.0.2)\r\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (0.9.6)\r\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (0.2.4)\r\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (2.0.1)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (0.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (1.0.0)\r\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (7.0.4)\r\n",
      "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /opt/conda/lib/python3.6/site-packages (from spacy<2.2,>=2.0->allennlp) (2.6.0)\r\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (0.14)\r\n",
      "Collecting rsa<=3.5.0,>=3.1.2 (from awscli>=1.11.91->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 17.8MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: colorama<=0.3.9,>=0.2.5 in /opt/conda/lib/python3.6/site-packages (from awscli>=1.11.91->allennlp) (0.3.9)\r\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\r\n",
      "Requirement already satisfied: sphinx>=1.2.3 in /opt/conda/lib/python3.6/site-packages (from numpydoc>=0.8.0->allennlp) (1.7.4)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (2019.3.9)\r\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.18->allennlp) (1.22)\r\n",
      "Collecting greenlet>=0.4.14; platform_python_implementation == \"CPython\" (from gevent>=1.3.6->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl (41kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 19.5MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (2.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib>=2.2.3->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: html5lib in /opt/conda/lib/python3.6/site-packages (from ftfy->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy->allennlp) (0.1.7)\r\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.6/site-packages (from mock->moto>=1.3.4->allennlp) (5.1.3)\r\n",
      "Collecting ecdsa<1.0 (from python-jose<4.0.0->moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/a8/8aa68e70959e1287da9154e5164bb8bd5dd7025e41ae54e8d177b8d165c9/ecdsa-0.13.2-py2.py3-none-any.whl (59kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 20.5MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: future<1.0 in /opt/conda/lib/python3.6/site-packages (from python-jose<4.0.0->moto>=1.3.4->allennlp) (0.17.1)\r\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp) (1.11.5)\r\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp) (0.24.0)\r\n",
      "Collecting docker-pycreds>=0.4.0 (from docker>=2.5.1->moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from docker>=2.5.1->moto>=1.3.4->allennlp) (0.56.0)\r\n",
      "Collecting aws-sam-translator>=1.10.0 (from cfn-lint->moto>=1.3.4->allennlp)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/dd/3f9c13d6454c4e59b71faacd19d7041e6d6becc76a8390aa2587f425df83/aws-sam-translator-1.11.0.tar.gz (96kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 23.6MB/s \r\n",
      "\u001b[?25hCollecting jsonpatch (from cfn-lint->moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/e6/d50d526ae2218b765ddbdb2dda14d65e19f501ce07410b375bc43ad20b7a/jsonpatch-1.23-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=2.10.1->moto>=1.3.4->allennlp) (1.0)\r\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.6/site-packages (from aws-xray-sdk!=0.96,>=0.93->moto>=1.3.4->allennlp) (1.10.11)\r\n",
      "Requirement already satisfied: jsonpickle in /opt/conda/lib/python3.6/site-packages (from aws-xray-sdk!=0.96,>=0.93->moto>=1.3.4->allennlp) (0.9.6)\r\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\r\n",
      "Requirement already satisfied: Pygments>=2.0 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (2.2.0)\r\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.2.1)\r\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (2.5.3)\r\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (0.7.10)\r\n",
      "Requirement already satisfied: imagesize in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.0.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (17.1)\r\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /opt/conda/lib/python3.6/site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.0.1)\r\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.6/site-packages (from html5lib->ftfy->allennlp) (0.5.1)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3.0->moto>=1.3.4->allennlp) (2.18)\r\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch->cfn-lint->moto>=1.3.4->allennlp)\r\n",
      "  Downloading https://files.pythonhosted.org/packages/18/b0/a80d29577c08eea401659254dfaed87f1af45272899e1812d7e01b679bc5/jsonpointer-2.0-py2.py3-none-any.whl\r\n",
      "Building wheels for collected packages: parsimonious, word2number, jsonnet, overrides, jsondiff, aws-sam-translator\r\n",
      "  Building wheel for parsimonious (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\r\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\r\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/f0/47/51/a178b15274ed0db775a1ae9c799ce31e511609c3ab75a7dec5\r\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\r\n",
      "  Building wheel for jsondiff (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/46/5f/86/11c6b72b064888e80b98bfcbcdaf2a83517a8cf8f2bb2a3227\r\n",
      "  Building wheel for aws-sam-translator (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/12/39/44/8528735e5e1989a5b19b4e2230d63e92847903cee1ab94fe6e\r\n",
      "Successfully built parsimonious word2number jsonnet overrides jsondiff aws-sam-translator\r\n",
      "\u001b[31mawscli 1.16.152 has requirement botocore==1.12.142, but you'll have botocore 1.12.134 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: msgpack, flaky, parsimonious, word2number, ecdsa, rsa, python-jose, cryptography, xmltodict, docker-pycreds, docker, jsondiff, responses, aws-sam-translator, jsonpointer, jsonpatch, cfn-lint, Jinja2, aws-xray-sdk, moto, flask-cors, conllu, awscli, jsonnet, sqlparse, greenlet, gevent, overrides, editdistance, allennlp\r\n",
      "  Found existing installation: msgpack 0.6.1\r\n",
      "    Uninstalling msgpack-0.6.1:\r\n",
      "      Successfully uninstalled msgpack-0.6.1\r\n",
      "  Found existing installation: rsa 4.0\r\n",
      "    Uninstalling rsa-4.0:\r\n",
      "      Successfully uninstalled rsa-4.0\r\n",
      "  Found existing installation: cryptography 2.2.2\r\n",
      "    Uninstalling cryptography-2.2.2:\r\n",
      "      Successfully uninstalled cryptography-2.2.2\r\n",
      "  Found existing installation: Jinja2 2.10\r\n",
      "    Uninstalling Jinja2-2.10:\r\n",
      "      Successfully uninstalled Jinja2-2.10\r\n",
      "  Found existing installation: Flask-Cors 3.0.4\r\n",
      "    Uninstalling Flask-Cors-3.0.4:\r\n",
      "      Successfully uninstalled Flask-Cors-3.0.4\r\n",
      "Successfully installed Jinja2-2.10.1 allennlp-0.8.3 aws-sam-translator-1.11.0 aws-xray-sdk-2.4.2 awscli-1.16.152 cfn-lint-0.19.1 conllu-0.11 cryptography-2.6.1 docker-3.7.2 docker-pycreds-0.4.0 ecdsa-0.13.2 editdistance-0.5.3 flaky-3.5.3 flask-cors-3.0.7 gevent-1.4.0 greenlet-0.4.15 jsondiff-1.1.2 jsonnet-0.12.1 jsonpatch-1.23 jsonpointer-2.0 moto-1.3.8 msgpack-0.5.6 overrides-1.9 parsimonious-0.8.1 python-jose-3.0.1 responses-0.10.6 rsa-3.4.2 sqlparse-0.3.0 word2number-1.1 xmltodict-0.12.0\r\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n",
      "Collecting https://github.com/ceshine/pytorch_helper_bot/archive/0.0.5.zip\r\n",
      "  Downloading https://github.com/ceshine/pytorch_helper_bot/archive/0.0.5.zip\r\n",
      "\u001b[K     - 133kB 8.0MB/s\r\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from PyTorchHelperBot==0.0.5) (1.0.1.post2)\r\n",
      "Building wheels for collected packages: PyTorchHelperBot\r\n",
      "  Building wheel for PyTorchHelperBot (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-b_ng77cl/wheels/83/9c/cd/789072117df3c77e5fd6368c30b293947b88b92a44034fee3b\r\n",
      "Successfully built PyTorchHelperBot\r\n",
      "Installing collected packages: PyTorchHelperBot\r\n",
      "Successfully installed PyTorchHelperBot-0.0.5\r\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!conda remove -y greenlet\n",
    "!pip install allennlp\n",
    "!pip install https://github.com/ceshine/pytorch_helper_bot/archive/0.0.5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.span_extractors import SelfAttentiveSpanExtractor, EndpointSpanExtractor\n",
    "\n",
    "from helperbot import (\n",
    "    TriangularLR, BaseBot, WeightDecayOptimizerWrapper,\n",
    "    GradualWarmupScheduler\n",
    ")\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"The MLP submodule\"\"\"\n",
    "    def __init__(self, bert_hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.bert_hidden_size = bert_hidden_size\n",
    "        self.span_extractor = SelfAttentiveSpanExtractor(bert_hidden_size * 3)\n",
    "#         self.span_extractor = EndpointSpanExtractor(\n",
    "#             bert_hidden_size * 3, \"x,y,x*y\"\n",
    "#         )\n",
    "        self.fc = nn.Sequential(\n",
    "            #nn.BatchNorm1d(bert_hidden_size * 3),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(bert_hidden_size * 3 * 3, 512),           \n",
    "            nn.ReLU(),\n",
    "            #nn.BatchNorm1d(512),      \n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(512, 3)\n",
    "        )\n",
    "#         for i, module in enumerate(self.fc):\n",
    "#             if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "#                 nn.init.constant_(module.weight, 1)\n",
    "#                 nn.init.constant_(module.bias, 0)\n",
    "#                 print(\"Initing batchnorm\")\n",
    "#             elif isinstance(module, nn.Linear):\n",
    "#                 if getattr(module, \"weight_v\", None) is not None:\n",
    "#                     nn.init.uniform_(module.weight_g, 0, 1)\n",
    "#                     nn.init.kaiming_normal_(module.weight_v)\n",
    "#                     print(\"Initing linear with weight normalization\")\n",
    "#                     assert model[i].weight_g is not None\n",
    "#                 else:\n",
    "#                     nn.init.kaiming_normal_(module.weight)\n",
    "#                     print(\"Initing linear\")\n",
    "#                 nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, bert_outputs, offsets):\n",
    "        assert bert_outputs.size(2) == self.bert_hidden_size * 3\n",
    "        spans_contexts = self.span_extractor(\n",
    "            bert_outputs, \n",
    "            offsets[:, :4].reshape(-1, 2, 2)\n",
    "        )\n",
    "        #print(f\"span_c shape: {spans_contexts.shape}\")\n",
    "        spans_contexts = spans_contexts.reshape(offsets.size()[0], -1)\n",
    "        #print(f\"span_c reshape: {spans_contexts.shape}\")\n",
    "        return self.fc(torch.cat([\n",
    "            spans_contexts,\n",
    "            torch.gather(\n",
    "                bert_outputs, 1,\n",
    "                offsets[:, [4]].unsqueeze(2).expand(-1, -1, self.bert_hidden_size * 3)\n",
    "            ).squeeze(1)\n",
    "        ], dim=1))\n",
    "\n",
    "class GAPDataset(Dataset):\n",
    "    \"\"\"Custom GAP Dataset class\"\"\"\n",
    "    def __init__(self, df, tokenizer, tokenize_fn, labeled=True):\n",
    "        self.labeled = labeled\n",
    "        if labeled:\n",
    "            self.y = df.target.values.astype(\"uint8\")\n",
    "        \n",
    "        self.offsets, self.tokens = [], []\n",
    "        for _, row in df.iterrows():\n",
    "            tokens, offsets = tokenize_fn(row, tokenizer)\n",
    "            self.offsets.append(offsets)\n",
    "            self.tokens.append(tokenizer.convert_tokens_to_ids(\n",
    "                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labeled:\n",
    "            return self.tokens[idx], self.offsets[idx], self.y[idx]\n",
    "        return self.tokens[idx], self.offsets[idx], None\n",
    "\n",
    "class GAPModel(nn.Module):\n",
    "    \"\"\"The main model.\"\"\"\n",
    "    def __init__(self, bert_model: str, device: torch.device, use_layer: int = -1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.use_layer = use_layer\n",
    "        if bert_model in (\"bert-base-uncased\", \"bert-base-cased\"):\n",
    "            self.bert_hidden_size = 768\n",
    "        elif bert_model in (\"bert-large-uncased\", \"bert-large-cased\"):\n",
    "            self.bert_hidden_size = 1024\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported BERT model.\")\n",
    "        self.bert = BertModel.from_pretrained(bert_model).to(device)\n",
    "        self.head = Head(self.bert_hidden_size).to(device)\n",
    "    \n",
    "    def forward(self, token_tensor, offsets):\n",
    "        token_tensor = token_tensor.to(self.device)\n",
    "        bert_outputs, _ =  self.bert(\n",
    "            token_tensor, attention_mask=(token_tensor > 0).long(), \n",
    "            token_type_ids=None, output_all_encoded_layers=True)\n",
    "        concat_bert = torch.cat((bert_outputs[-1],bert_outputs[-2],bert_outputs[-3]),dim=-1)\n",
    "        #head_outputs = self.head(bert_outputs[self.use_layer], offsets.to(self.device))\n",
    "        head_outputs = self.head(concat_bert, offsets.to(self.device))\n",
    "        return head_outputs            \n",
    "    \n",
    "class GAPBot(BaseBot):\n",
    "    def __init__(self, model, train_loader, val_loader, *, optimizer, clip_grad=0,\n",
    "        avg_window=100, log_dir=\"./cache/logs/\", log_level=logging.INFO,\n",
    "        checkpoint_dir=\"./cache/model_cache/\", batch_idx=0, echo=False,\n",
    "        device=\"cuda:0\", use_tensorboard=False):\n",
    "        super().__init__(\n",
    "            model, train_loader, val_loader, \n",
    "            optimizer=optimizer, clip_grad=clip_grad,\n",
    "            log_dir=log_dir, checkpoint_dir=checkpoint_dir, \n",
    "            batch_idx=batch_idx, echo=echo,\n",
    "            device=device, use_tensorboard=use_tensorboard\n",
    "        )\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.loss_format = \"%.6f\"\n",
    "        \n",
    "    def extract_prediction(self, tensor):\n",
    "        return tensor\n",
    "    \n",
    "    def snapshot(self):\n",
    "        \"\"\"Override the snapshot method because Kaggle kernel has limited local disk space.\"\"\"\n",
    "        loss = self.eval(self.val_loader)\n",
    "        loss_str = self.loss_format % loss\n",
    "        self.logger.info(\"Snapshot loss %s\", loss_str)\n",
    "        self.logger.tb_scalars(\n",
    "            \"losses\", {\"val\": loss},  self.step)\n",
    "        target_path = (\n",
    "            self.checkpoint_dir / \"best.pth\")        \n",
    "        if not self.best_performers or (self.best_performers[0][0] > loss):\n",
    "            torch.save(self.model.state_dict(), target_path)\n",
    "            self.best_performers = [(loss, target_path, self.step)]\n",
    "        self.logger.info(\"Saving checkpoint %s...\", target_path)\n",
    "        assert Path(target_path).exists()\n",
    "        return loss\n",
    "\n",
    "class BERTSpanExtractor:\n",
    "    def __init__(self, dev_df, val_df, test_df, bert_model = 'bert-large-uncased', do_lower_case=True, learning_rate=1e-5, n_epochs=30,\n",
    "                train_batch_size=10, predict_batch_size=32):\n",
    "        self.dev_df = self.extract_target(dev_df)\n",
    "        self.val_df = self.extract_target(val_df)\n",
    "        self.test_df = self.extract_target(test_df)\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.predict_batch_size = predict_batch_size\n",
    "        self.bert_model = bert_model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_model, do_lower_case=self.do_lower_case, \n",
    "                                                       never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\"))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    def extract_target(self, df):\n",
    "#         df[\"Neither\"] = 0\n",
    "#         df.loc[~(df['A-coref'] | df['B-coref']), \"Neither\"] = 1\n",
    "#         df[\"target\"] = 0\n",
    "#         df.loc[df['B-coref'] == 1, \"target\"] = 1\n",
    "#         df.loc[df[\"Neither\"] == 1, \"target\"] = 2\n",
    "        #print(df.target.value_counts())\n",
    "        df['target'] = [get_class_label(aco, bco) for aco, bco in zip(df['A-coref'], df['B-coref'])]\n",
    "        df['gender'] = df['Pronoun'].transform(get_gender)\n",
    "\n",
    "        return df\n",
    "        \n",
    "    def tokenize(self, row, tokenizer):\n",
    "        break_points = sorted(\n",
    "            [\n",
    "                (\"A\", row[\"A-offset\"], row[\"A\"]),\n",
    "                (\"B\", row[\"B-offset\"], row[\"B\"]),\n",
    "                (\"P\", row[\"Pronoun-offset\"], row[\"Pronoun\"]),\n",
    "            ], key=lambda x: x[0]\n",
    "        )\n",
    "        tokens, spans, current_pos = [], {}, 0\n",
    "        for name, offset, text in break_points:\n",
    "            tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n",
    "            # Make sure we do not get it wrong\n",
    "            assert row[\"Text\"][offset:offset+len(text)] == text\n",
    "            # Tokenize the target\n",
    "            tmp_tokens = tokenizer.tokenize(row[\"Text\"][offset:offset+len(text)])\n",
    "            spans[name] = [len(tokens), len(tokens) + len(tmp_tokens) - 1] # inclusive\n",
    "            tokens.extend(tmp_tokens)\n",
    "            current_pos = offset + len(text)\n",
    "        tokens.extend(tokenizer.tokenize(row[\"Text\"][current_pos:offset]))\n",
    "        assert spans[\"P\"][0] == spans[\"P\"][1]\n",
    "        return tokens, (spans[\"A\"] + spans[\"B\"] + [spans[\"P\"][0]])\n",
    "\n",
    "    def collate_examples(self, batch, truncate_len=490):\n",
    "        \"\"\"Batch preparation.\n",
    "\n",
    "        1. Pad the sequences\n",
    "        2. Transform the target.\n",
    "        \"\"\"    \n",
    "        transposed = list(zip(*batch))\n",
    "        max_len = min(\n",
    "            max((len(x) for x in transposed[0])),\n",
    "            truncate_len\n",
    "        )\n",
    "        tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n",
    "        for i, row in enumerate(transposed[0]):\n",
    "            row = np.array(row[:truncate_len])\n",
    "            tokens[i, :len(row)] = row\n",
    "        token_tensor = torch.from_numpy(tokens)\n",
    "        # Offsets\n",
    "        offsets = torch.stack([\n",
    "            torch.LongTensor(x) for x in transposed[1]\n",
    "        ], dim=0) + 1 # Account for the [CLS] token\n",
    "        # Labels\n",
    "        if len(transposed) == 2 or transposed[2][0] is None:\n",
    "            return token_tensor, offsets, None\n",
    "        labels = torch.LongTensor(transposed[2])\n",
    "        return token_tensor, offsets, labels\n",
    "    \n",
    "    def run_k_fold(self):\n",
    "        test_ds = GAPDataset(self.test_df, self.tokenizer, self.tokenize, labeled=True) #not great, but it's a hack needed so this \"bot\" thing doesn't crash\n",
    "        test_loader = DataLoader(\n",
    "            test_ds,\n",
    "            collate_fn = self.collate_examples,\n",
    "            batch_size=self.predict_batch_size,\n",
    "            num_workers=2,\n",
    "            pin_memory=True,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        kfold_data = pd.concat([self.dev_df, self.val_df])\n",
    "        kf = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)\n",
    "\n",
    "        val_preds, test_preds, val_ys, val_losses = [], [], [], []\n",
    "        for train_index, valid_index in kf.split(kfold_data, kfold_data[\"gender\"]):\n",
    "            print(\"=\" * 20)\n",
    "            print(f\"Fold {len(val_preds) + 1}\")\n",
    "            print(\"=\" * 20)\n",
    "            train_ds = GAPDataset(kfold_data.iloc[train_index], self.tokenizer, self.tokenize)\n",
    "            val_ds = GAPDataset(kfold_data.iloc[valid_index], self.tokenizer, self.tokenize)\n",
    "            train_loader = DataLoader(\n",
    "                train_ds,\n",
    "                collate_fn = self.collate_examples,\n",
    "                batch_size=self.train_batch_size,\n",
    "                num_workers=2,\n",
    "                pin_memory=True,\n",
    "                shuffle=True,\n",
    "                drop_last=False #True\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_ds,\n",
    "                collate_fn = self.collate_examples,\n",
    "                batch_size=self.predict_batch_size,\n",
    "                num_workers=2,\n",
    "                pin_memory=True,\n",
    "                shuffle=False\n",
    "            )\n",
    "            model = GAPModel(self.bert_model, self.device)\n",
    "            \n",
    "            def children(m):\n",
    "                return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "            def set_trainable_attr(m, b):\n",
    "                m.trainable = b\n",
    "                for p in m.parameters():\n",
    "                    p.requires_grad = b\n",
    "\n",
    "\n",
    "            def apply_leaf(m, f):\n",
    "                c = children(m)\n",
    "                if isinstance(m, nn.Module):\n",
    "                    f(m)\n",
    "                if len(c) > 0:\n",
    "                    for l in c:\n",
    "                        apply_leaf(l, f)\n",
    "\n",
    "\n",
    "            def set_trainable(l, b):\n",
    "                apply_leaf(l, lambda m: set_trainable_attr(m, b))\n",
    "                \n",
    "            # You can unfreeze the last layer of bert by calling set_trainable(model.bert.encoder.layer[23], True)\n",
    "            set_trainable(model.bert, False)\n",
    "            set_trainable(model.head, True)\n",
    "            for i in range(12,24):\n",
    "                set_trainable(model.bert.encoder.layer[i], True)\n",
    "\n",
    "        #     optimizer = WeightDecayOptimizerWrapper(\n",
    "        #         torch.optim.Adam(model.parameters(), lr=1e-5),\n",
    "        #         0.05\n",
    "        #     )\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "            # optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "            bot = GAPBot(\n",
    "                model, train_loader, val_loader,\n",
    "                optimizer=optimizer, echo=True,\n",
    "                avg_window=25\n",
    "            )\n",
    "            gc.collect()\n",
    "            steps_per_epoch = len(train_loader) \n",
    "            n_steps = steps_per_epoch * self.n_epochs\n",
    "            bot.train(\n",
    "                n_steps,\n",
    "                log_interval=steps_per_epoch // 2,\n",
    "                snapshot_interval=steps_per_epoch,\n",
    "        #         scheduler=GradualWarmupScheduler(optimizer, 20, int(steps_per_epoch * 4),\n",
    "        #             after_scheduler=CosineAnnealingLR(\n",
    "        #                 optimizer, n_steps - int(steps_per_epoch * 4)\n",
    "        #             )\n",
    "        #         )\n",
    "                scheduler=TriangularLR(\n",
    "                    optimizer, 20, ratio=2, steps_per_cycle=steps_per_epoch * 100)\n",
    "            )\n",
    "            # Load the best checkpoint\n",
    "            bot.load_model(bot.best_performers[0][1])\n",
    "            bot.remove_checkpoints(keep=0)    \n",
    "            val_preds.append(torch.softmax(bot.predict(val_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())\n",
    "            val_ys.append(kfold_data.iloc[valid_index].target.astype(\"uint8\").values)\n",
    "            val_losses.append(log_loss(val_ys[-1], val_preds[-1]))\n",
    "            bot.logger.info(\"Confirm val loss: %.4f\", val_losses[-1])\n",
    "            test_preds.append(torch.softmax(bot.predict(test_loader), -1).clamp(1e-4, 1-1e-4).cpu().numpy())\n",
    "            del model\n",
    "            \n",
    "        return val_preds, test_preds, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_prod['A-coref'] = [True] * len(test_df_prod)\n",
    "test_df_prod['B-coref'] = [False] * len(test_df_prod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2019 00:43:25 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "05/04/2019 00:43:25 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt not found in cache, downloading to /tmp/tmp14zjkk18\n",
      "100%|██████████| 213450/213450 [00:00<00:00, 5434731.50B/s]\n",
      "05/04/2019 00:43:25 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmp14zjkk18 to cache at /tmp/.pytorch_pretrained_bert/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "05/04/2019 00:43:25 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /tmp/.pytorch_pretrained_bert/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "05/04/2019 00:43:25 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmp14zjkk18\n",
      "05/04/2019 00:43:25 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt from cache at /tmp/.pytorch_pretrained_bert/cee054f6aafe5e2cf816d2228704e326446785f940f5451a5b26033516a4ac3d.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 1\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2019 00:43:53 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz not found in cache, downloading to /tmp/tmpff1dbxmx\n",
      "100%|██████████| 1242874899/1242874899 [00:30<00:00, 41347699.25B/s]\n",
      "05/04/2019 00:44:23 - INFO - pytorch_pretrained_bert.file_utils -   copying /tmp/tmpff1dbxmx to cache at /tmp/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233\n",
      "05/04/2019 00:44:26 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for /tmp/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233\n",
      "05/04/2019 00:44:26 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file /tmp/tmpff1dbxmx\n",
      "05/04/2019 00:44:27 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz from cache at /tmp/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233\n",
      "05/04/2019 00:44:27 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /tmp/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233 to temp dir /tmp/tmp_gcrwewk\n",
      "05/04/2019 00:44:50 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[[05/04/2019 12:45:05 AM]] SEED: 9293\n",
      "[[05/04/2019 12:45:05 AM]] # of paramters: 338,302,980\n",
      "[[05/04/2019 12:45:05 AM]] # of trainable paramters: 155,878,404\n",
      "[[05/04/2019 12:45:05 AM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 1e-05\n",
      "    lr: 5.000000000000001e-07\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[05/04/2019 12:45:05 AM]] Batches per epoch: 357\n",
      "[[05/04/2019 12:45:05 AM]] ====================Epoch 1====================\n",
      "[[05/04/2019 12:46:08 AM]] Step 178: train 1.026822 lr: 6.405e-07\n",
      "[[05/04/2019 12:47:07 AM]] Step 356: train 0.956209 lr: 7.826e-07\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 12:47:23 AM]] Snapshot loss 0.894441\n",
      "[[05/04/2019 12:47:25 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 12:47:25 AM]] New low\n",
      "\n",
      "[[05/04/2019 12:47:25 AM]] ====================Epoch 2====================\n",
      "[[05/04/2019 12:48:25 AM]] Step 534: train 0.879661 lr: 9.247e-07\n",
      "[[05/04/2019 12:49:25 AM]] Step 712: train 0.816633 lr: 1.067e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 12:49:41 AM]] Snapshot loss 0.758971\n",
      "[[05/04/2019 12:49:43 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 12:49:43 AM]] New low\n",
      "\n",
      "[[05/04/2019 12:49:43 AM]] ====================Epoch 3====================\n",
      "[[05/04/2019 12:50:44 AM]] Step 890: train 0.744809 lr: 1.209e-06\n",
      "[[05/04/2019 12:51:44 AM]] Step 1068: train 0.667023 lr: 1.351e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 12:52:00 AM]] Snapshot loss 0.608487\n",
      "[[05/04/2019 12:52:02 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 12:52:02 AM]] New low\n",
      "\n",
      "[[05/04/2019 12:52:02 AM]] ====================Epoch 4====================\n",
      "[[05/04/2019 12:53:01 AM]] Step 1246: train 0.579213 lr: 1.493e-06\n",
      "[[05/04/2019 12:54:01 AM]] Step 1424: train 0.495074 lr: 1.635e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 12:54:18 AM]] Snapshot loss 0.447461\n",
      "[[05/04/2019 12:54:20 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 12:54:20 AM]] New low\n",
      "\n",
      "[[05/04/2019 12:54:20 AM]] ====================Epoch 5====================\n",
      "[[05/04/2019 12:55:20 AM]] Step 1602: train 0.407035 lr: 1.777e-06\n",
      "[[05/04/2019 12:56:20 AM]] Step 1780: train 0.344487 lr: 1.919e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 12:56:37 AM]] Snapshot loss 0.428494\n",
      "[[05/04/2019 12:56:38 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 12:56:38 AM]] New low\n",
      "\n",
      "[[05/04/2019 12:56:39 AM]] ====================Epoch 6====================\n",
      "[[05/04/2019 12:57:38 AM]] Step 1958: train 0.302525 lr: 2.062e-06\n",
      "[[05/04/2019 12:58:38 AM]] Step 2136: train 0.271336 lr: 2.204e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 12:58:55 AM]] Snapshot loss 0.427088\n",
      "[[05/04/2019 12:58:57 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 12:58:57 AM]] New low\n",
      "\n",
      "[[05/04/2019 12:58:57 AM]] ====================Epoch 7====================\n",
      "[[05/04/2019 12:59:55 AM]] Step 2314: train 0.250499 lr: 2.346e-06\n",
      "[[05/04/2019 01:00:56 AM]] Step 2492: train 0.221126 lr: 2.488e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 01:01:13 AM]] Snapshot loss 0.431844\n",
      "[[05/04/2019 01:01:13 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:01:13 AM]] ====================Epoch 8====================\n",
      "[[05/04/2019 01:02:10 AM]] Step 2670: train 0.175422 lr: 2.630e-06\n",
      "[[05/04/2019 01:03:11 AM]] Step 2848: train 0.162096 lr: 2.772e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 01:03:29 AM]] Snapshot loss 0.453899\n",
      "[[05/04/2019 01:03:29 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:03:29 AM]] ====================Epoch 9====================\n",
      "[[05/04/2019 01:04:26 AM]] Step 3026: train 0.143285 lr: 2.914e-06\n",
      "[[05/04/2019 01:05:27 AM]] Step 3204: train 0.116292 lr: 3.056e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 01:05:46 AM]] Snapshot loss 0.478719\n",
      "[[05/04/2019 01:05:46 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:05:46 AM]] ====================Epoch 10====================\n",
      "[[05/04/2019 01:06:43 AM]] Step 3382: train 0.103819 lr: 3.198e-06\n",
      "[[05/04/2019 01:07:43 AM]] Step 3560: train 0.086846 lr: 3.340e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 01:08:02 AM]] Snapshot loss 0.518627\n",
      "[[05/04/2019 01:08:02 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:08:02 AM]] ====================Epoch 11====================\n",
      "[[05/04/2019 01:08:59 AM]] Step 3738: train 0.067998 lr: 3.483e-06\n",
      "[[05/04/2019 01:10:00 AM]] Step 3916: train 0.060260 lr: 3.625e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 01:10:18 AM]] Snapshot loss 0.576761\n",
      "[[05/04/2019 01:10:18 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:10:18 AM]] ====================Epoch 12====================\n",
      "[[05/04/2019 01:11:15 AM]] Step 4094: train 0.054189 lr: 3.767e-06\n",
      "[[05/04/2019 01:12:15 AM]] Step 4272: train 0.041528 lr: 3.909e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 01:12:35 AM]] Snapshot loss 0.609965\n",
      "[[05/04/2019 01:12:35 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:12:35 AM]] ====================Epoch 13====================\n",
      "[[05/04/2019 01:13:31 AM]] Step 4450: train 0.032980 lr: 4.051e-06\n",
      "[[05/04/2019 01:14:32 AM]] Step 4628: train 0.020074 lr: 4.193e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 01:14:51 AM]] Snapshot loss 0.679066\n",
      "[[05/04/2019 01:14:51 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:14:51 AM]] ====================Epoch 14====================\n",
      "[[05/04/2019 01:15:46 AM]] Step 4806: train 0.014717 lr: 4.335e-06\n",
      "[[05/04/2019 01:16:48 AM]] Step 4984: train 0.021366 lr: 4.477e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 01:17:08 AM]] Snapshot loss 0.655538\n",
      "[[05/04/2019 01:17:08 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:17:08 AM]] ====================Epoch 15====================\n",
      "[[05/04/2019 01:18:04 AM]] Step 5162: train 0.019158 lr: 4.619e-06\n",
      "[[05/04/2019 01:19:04 AM]] Step 5340: train 0.018677 lr: 4.761e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.28it/s]\n",
      "[[05/04/2019 01:19:25 AM]] Snapshot loss 0.790661\n",
      "[[05/04/2019 01:19:25 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:19:41 AM]] Confirm val loss: 0.4271\n",
      "100%|██████████| 387/387 [02:20<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 2\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2019 01:22:10 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz from cache at /tmp/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233\n",
      "05/04/2019 01:22:10 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /tmp/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233 to temp dir /tmp/tmpzm8rx_l1\n",
      "05/04/2019 01:22:24 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[[05/04/2019 01:22:34 AM]] SEED: 9293\n",
      "[[05/04/2019 01:22:34 AM]] # of paramters: 338,302,980\n",
      "[[05/04/2019 01:22:34 AM]] # of trainable paramters: 155,878,404\n",
      "[[05/04/2019 01:22:35 AM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 1e-05\n",
      "    lr: 5.000000000000001e-07\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[05/04/2019 01:22:35 AM]] Batches per epoch: 357\n",
      "[[05/04/2019 01:22:35 AM]] ====================Epoch 1====================\n",
      "[[05/04/2019 01:23:34 AM]] Step 178: train 1.010478 lr: 6.405e-07\n",
      "[[05/04/2019 01:24:36 AM]] Step 356: train 0.950879 lr: 7.826e-07\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:24:52 AM]] Snapshot loss 0.895115\n",
      "[[05/04/2019 01:24:54 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:24:54 AM]] New low\n",
      "\n",
      "[[05/04/2019 01:24:54 AM]] ====================Epoch 2====================\n",
      "[[05/04/2019 01:25:55 AM]] Step 534: train 0.889466 lr: 9.247e-07\n",
      "[[05/04/2019 01:26:55 AM]] Step 712: train 0.815564 lr: 1.067e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:27:11 AM]] Snapshot loss 0.745668\n",
      "[[05/04/2019 01:27:13 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:27:13 AM]] New low\n",
      "\n",
      "[[05/04/2019 01:27:13 AM]] ====================Epoch 3====================\n",
      "[[05/04/2019 01:28:16 AM]] Step 890: train 0.737569 lr: 1.209e-06\n",
      "[[05/04/2019 01:29:16 AM]] Step 1068: train 0.659983 lr: 1.351e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:29:32 AM]] Snapshot loss 0.587973\n",
      "[[05/04/2019 01:29:34 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:29:34 AM]] New low\n",
      "\n",
      "[[05/04/2019 01:29:34 AM]] ====================Epoch 4====================\n",
      "[[05/04/2019 01:30:34 AM]] Step 1246: train 0.562086 lr: 1.493e-06\n",
      "[[05/04/2019 01:31:35 AM]] Step 1424: train 0.486378 lr: 1.635e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:31:51 AM]] Snapshot loss 0.405736\n",
      "[[05/04/2019 01:31:53 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:31:53 AM]] New low\n",
      "\n",
      "[[05/04/2019 01:31:53 AM]] ====================Epoch 5====================\n",
      "[[05/04/2019 01:32:52 AM]] Step 1602: train 0.405332 lr: 1.777e-06\n",
      "[[05/04/2019 01:33:52 AM]] Step 1780: train 0.338400 lr: 1.919e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:34:09 AM]] Snapshot loss 0.365421\n",
      "[[05/04/2019 01:34:11 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:34:11 AM]] New low\n",
      "\n",
      "[[05/04/2019 01:34:11 AM]] ====================Epoch 6====================\n",
      "[[05/04/2019 01:35:11 AM]] Step 1958: train 0.299607 lr: 2.062e-06\n",
      "[[05/04/2019 01:36:11 AM]] Step 2136: train 0.282857 lr: 2.204e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:36:28 AM]] Snapshot loss 0.344377\n",
      "[[05/04/2019 01:36:30 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:36:30 AM]] New low\n",
      "\n",
      "[[05/04/2019 01:36:30 AM]] ====================Epoch 7====================\n",
      "[[05/04/2019 01:37:29 AM]] Step 2314: train 0.250499 lr: 2.346e-06\n",
      "[[05/04/2019 01:38:29 AM]] Step 2492: train 0.219436 lr: 2.488e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:38:47 AM]] Snapshot loss 0.365495\n",
      "[[05/04/2019 01:38:47 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:38:47 AM]] ====================Epoch 8====================\n",
      "[[05/04/2019 01:39:46 AM]] Step 2670: train 0.178446 lr: 2.630e-06\n",
      "[[05/04/2019 01:40:46 AM]] Step 2848: train 0.165857 lr: 2.772e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:41:03 AM]] Snapshot loss 0.360291\n",
      "[[05/04/2019 01:41:03 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:41:03 AM]] ====================Epoch 9====================\n",
      "[[05/04/2019 01:42:00 AM]] Step 3026: train 0.149100 lr: 2.914e-06\n",
      "[[05/04/2019 01:43:01 AM]] Step 3204: train 0.119349 lr: 3.056e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:43:19 AM]] Snapshot loss 0.391179\n",
      "[[05/04/2019 01:43:19 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:43:19 AM]] ====================Epoch 10====================\n",
      "[[05/04/2019 01:44:16 AM]] Step 3382: train 0.108021 lr: 3.198e-06\n",
      "[[05/04/2019 01:45:18 AM]] Step 3560: train 0.088823 lr: 3.340e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:45:36 AM]] Snapshot loss 0.440073\n",
      "[[05/04/2019 01:45:36 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:45:36 AM]] ====================Epoch 11====================\n",
      "[[05/04/2019 01:46:33 AM]] Step 3738: train 0.066234 lr: 3.483e-06\n",
      "[[05/04/2019 01:47:34 AM]] Step 3916: train 0.057145 lr: 3.625e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:47:52 AM]] Snapshot loss 0.460513\n",
      "[[05/04/2019 01:47:52 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:47:52 AM]] ====================Epoch 12====================\n",
      "[[05/04/2019 01:48:50 AM]] Step 4094: train 0.047929 lr: 3.767e-06\n",
      "[[05/04/2019 01:49:50 AM]] Step 4272: train 0.040252 lr: 3.909e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:50:09 AM]] Snapshot loss 0.498136\n",
      "[[05/04/2019 01:50:09 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:50:09 AM]] ====================Epoch 13====================\n",
      "[[05/04/2019 01:51:06 AM]] Step 4450: train 0.032028 lr: 4.051e-06\n",
      "[[05/04/2019 01:52:06 AM]] Step 4628: train 0.024729 lr: 4.193e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:52:26 AM]] Snapshot loss 0.551274\n",
      "[[05/04/2019 01:52:26 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:52:26 AM]] ====================Epoch 14====================\n",
      "[[05/04/2019 01:53:22 AM]] Step 4806: train 0.025433 lr: 4.335e-06\n",
      "[[05/04/2019 01:54:22 AM]] Step 4984: train 0.020615 lr: 4.477e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:54:42 AM]] Snapshot loss 0.557241\n",
      "[[05/04/2019 01:54:42 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 01:54:42 AM]] ====================Epoch 15====================\n",
      "[[05/04/2019 01:55:38 AM]] Step 5162: train 0.023594 lr: 4.619e-06\n",
      "[[05/04/2019 01:56:39 AM]] Step 5340: train 0.020899 lr: 4.761e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.26it/s]\n",
      "[[05/04/2019 01:56:59 AM]] Snapshot loss 0.598716\n",
      "[[05/04/2019 01:56:59 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.25it/s]\n",
      "[[05/04/2019 01:57:15 AM]] Confirm val loss: 0.3444\n",
      "100%|██████████| 387/387 [02:20<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 3\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2019 01:59:43 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz from cache at /tmp/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233\n",
      "05/04/2019 01:59:43 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /tmp/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233 to temp dir /tmp/tmpq1uvfpzk\n",
      "05/04/2019 01:59:58 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[[05/04/2019 02:00:08 AM]] SEED: 9293\n",
      "[[05/04/2019 02:00:08 AM]] # of paramters: 338,302,980\n",
      "[[05/04/2019 02:00:08 AM]] # of trainable paramters: 155,878,404\n",
      "[[05/04/2019 02:00:08 AM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 1e-05\n",
      "    lr: 5.000000000000001e-07\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[05/04/2019 02:00:08 AM]] Batches per epoch: 357\n",
      "[[05/04/2019 02:00:08 AM]] ====================Epoch 1====================\n",
      "[[05/04/2019 02:01:09 AM]] Step 178: train 1.027126 lr: 6.405e-07\n",
      "[[05/04/2019 02:02:10 AM]] Step 356: train 0.968081 lr: 7.826e-07\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:02:24 AM]] Snapshot loss 0.897735\n",
      "[[05/04/2019 02:02:26 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:02:26 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:02:26 AM]] ====================Epoch 2====================\n",
      "[[05/04/2019 02:03:28 AM]] Step 534: train 0.895495 lr: 9.247e-07\n",
      "[[05/04/2019 02:04:29 AM]] Step 712: train 0.850299 lr: 1.067e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:04:43 AM]] Snapshot loss 0.788649\n",
      "[[05/04/2019 02:04:45 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:04:45 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:04:45 AM]] ====================Epoch 3====================\n",
      "[[05/04/2019 02:05:46 AM]] Step 890: train 0.785030 lr: 1.209e-06\n",
      "[[05/04/2019 02:06:47 AM]] Step 1068: train 0.724204 lr: 1.351e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:07:02 AM]] Snapshot loss 0.662006\n",
      "[[05/04/2019 02:07:04 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:07:04 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:07:04 AM]] ====================Epoch 4====================\n",
      "[[05/04/2019 02:08:04 AM]] Step 1246: train 0.656871 lr: 1.493e-06\n",
      "[[05/04/2019 02:09:06 AM]] Step 1424: train 0.581935 lr: 1.635e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:09:20 AM]] Snapshot loss 0.527893\n",
      "[[05/04/2019 02:09:23 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:09:23 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:09:23 AM]] ====================Epoch 5====================\n",
      "[[05/04/2019 02:10:22 AM]] Step 1602: train 0.505133 lr: 1.777e-06\n",
      "[[05/04/2019 02:11:24 AM]] Step 1780: train 0.434089 lr: 1.919e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:11:39 AM]] Snapshot loss 0.395868\n",
      "[[05/04/2019 02:11:41 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:11:41 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:11:41 AM]] ====================Epoch 6====================\n",
      "[[05/04/2019 02:12:42 AM]] Step 1958: train 0.367779 lr: 2.062e-06\n",
      "[[05/04/2019 02:13:44 AM]] Step 2136: train 0.332624 lr: 2.204e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:13:59 AM]] Snapshot loss 0.381449\n",
      "[[05/04/2019 02:14:01 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:14:01 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:14:01 AM]] ====================Epoch 7====================\n",
      "[[05/04/2019 02:15:04 AM]] Step 2314: train 0.283658 lr: 2.346e-06\n",
      "[[05/04/2019 02:16:04 AM]] Step 2492: train 0.277342 lr: 2.488e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:16:20 AM]] Snapshot loss 0.387711\n",
      "[[05/04/2019 02:16:20 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:16:20 AM]] ====================Epoch 8====================\n",
      "[[05/04/2019 02:17:18 AM]] Step 2670: train 0.245800 lr: 2.630e-06\n",
      "[[05/04/2019 02:18:21 AM]] Step 2848: train 0.205362 lr: 2.772e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:18:37 AM]] Snapshot loss 0.399890\n",
      "[[05/04/2019 02:18:37 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:18:37 AM]] ====================Epoch 9====================\n",
      "[[05/04/2019 02:19:35 AM]] Step 3026: train 0.185145 lr: 2.914e-06\n",
      "[[05/04/2019 02:20:37 AM]] Step 3204: train 0.161062 lr: 3.056e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:20:53 AM]] Snapshot loss 0.417799\n",
      "[[05/04/2019 02:20:53 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:20:53 AM]] ====================Epoch 10====================\n",
      "[[05/04/2019 02:21:51 AM]] Step 3382: train 0.138090 lr: 3.198e-06\n",
      "[[05/04/2019 02:22:53 AM]] Step 3560: train 0.116808 lr: 3.340e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:23:09 AM]] Snapshot loss 0.474664\n",
      "[[05/04/2019 02:23:09 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:23:09 AM]] ====================Epoch 11====================\n",
      "[[05/04/2019 02:24:07 AM]] Step 3738: train 0.097598 lr: 3.483e-06\n",
      "[[05/04/2019 02:25:09 AM]] Step 3916: train 0.077284 lr: 3.625e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:25:26 AM]] Snapshot loss 0.534768\n",
      "[[05/04/2019 02:25:26 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:25:26 AM]] ====================Epoch 12====================\n",
      "[[05/04/2019 02:26:24 AM]] Step 4094: train 0.060932 lr: 3.767e-06\n",
      "[[05/04/2019 02:27:24 AM]] Step 4272: train 0.042689 lr: 3.909e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:27:42 AM]] Snapshot loss 0.606961\n",
      "[[05/04/2019 02:27:42 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:27:42 AM]] ====================Epoch 13====================\n",
      "[[05/04/2019 02:28:39 AM]] Step 4450: train 0.039231 lr: 4.051e-06\n",
      "[[05/04/2019 02:29:41 AM]] Step 4628: train 0.038495 lr: 4.193e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.73it/s]\n",
      "[[05/04/2019 02:29:59 AM]] Snapshot loss 0.631246\n",
      "[[05/04/2019 02:29:59 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:29:59 AM]] ====================Epoch 14====================\n",
      "[[05/04/2019 02:30:55 AM]] Step 4806: train 0.026640 lr: 4.335e-06\n",
      "[[05/04/2019 02:31:57 AM]] Step 4984: train 0.020466 lr: 4.477e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.74it/s]\n",
      "[[05/04/2019 02:32:15 AM]] Snapshot loss 0.676382\n",
      "[[05/04/2019 02:32:15 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:32:15 AM]] ====================Epoch 15====================\n",
      "[[05/04/2019 02:33:11 AM]] Step 5162: train 0.023126 lr: 4.619e-06\n",
      "[[05/04/2019 02:34:12 AM]] Step 5340: train 0.023988 lr: 4.761e-06\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.73it/s]\n",
      "[[05/04/2019 02:34:31 AM]] Snapshot loss 0.689148\n",
      "[[05/04/2019 02:34:31 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 28/28 [00:13<00:00,  2.71it/s]\n",
      "[[05/04/2019 02:34:46 AM]] Confirm val loss: 0.3814\n",
      "100%|██████████| 387/387 [02:20<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 4\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2019 02:37:14 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz from cache at /tmp/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233\n",
      "05/04/2019 02:37:14 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /tmp/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233 to temp dir /tmp/tmpccxnohw7\n",
      "05/04/2019 02:37:28 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[[05/04/2019 02:37:38 AM]] SEED: 9293\n",
      "[[05/04/2019 02:37:38 AM]] # of paramters: 338,302,980\n",
      "[[05/04/2019 02:37:38 AM]] # of trainable paramters: 155,878,404\n",
      "[[05/04/2019 02:37:38 AM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 1e-05\n",
      "    lr: 5.000000000000001e-07\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[05/04/2019 02:37:38 AM]] Batches per epoch: 357\n",
      "[[05/04/2019 02:37:38 AM]] ====================Epoch 1====================\n",
      "[[05/04/2019 02:38:39 AM]] Step 178: train 1.044258 lr: 6.405e-07\n",
      "[[05/04/2019 02:39:40 AM]] Step 356: train 0.973580 lr: 7.826e-07\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 02:39:55 AM]] Snapshot loss 0.879057\n",
      "[[05/04/2019 02:39:57 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:39:57 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:39:57 AM]] ====================Epoch 2====================\n",
      "[[05/04/2019 02:40:58 AM]] Step 534: train 0.908756 lr: 9.247e-07\n",
      "[[05/04/2019 02:41:59 AM]] Step 712: train 0.853456 lr: 1.067e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 02:42:15 AM]] Snapshot loss 0.753746\n",
      "[[05/04/2019 02:42:17 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:42:17 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:42:17 AM]] ====================Epoch 3====================\n",
      "[[05/04/2019 02:43:16 AM]] Step 890: train 0.803016 lr: 1.209e-06\n",
      "[[05/04/2019 02:44:18 AM]] Step 1068: train 0.717881 lr: 1.351e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 02:44:34 AM]] Snapshot loss 0.620398\n",
      "[[05/04/2019 02:44:36 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:44:36 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:44:36 AM]] ====================Epoch 4====================\n",
      "[[05/04/2019 02:45:37 AM]] Step 1246: train 0.647376 lr: 1.493e-06\n",
      "[[05/04/2019 02:46:37 AM]] Step 1424: train 0.573920 lr: 1.635e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 02:46:53 AM]] Snapshot loss 0.471688\n",
      "[[05/04/2019 02:46:56 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:46:56 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:46:56 AM]] ====================Epoch 5====================\n",
      "[[05/04/2019 02:47:55 AM]] Step 1602: train 0.481771 lr: 1.777e-06\n",
      "[[05/04/2019 02:48:56 AM]] Step 1780: train 0.399582 lr: 1.919e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 02:49:13 AM]] Snapshot loss 0.365912\n",
      "[[05/04/2019 02:49:15 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:49:15 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:49:15 AM]] ====================Epoch 6====================\n",
      "[[05/04/2019 02:50:16 AM]] Step 1958: train 0.352226 lr: 2.062e-06\n",
      "[[05/04/2019 02:51:15 AM]] Step 2136: train 0.315782 lr: 2.204e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 02:51:32 AM]] Snapshot loss 0.353190\n",
      "[[05/04/2019 02:51:34 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:51:34 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:51:34 AM]] ====================Epoch 7====================\n",
      "[[05/04/2019 02:52:33 AM]] Step 2314: train 0.272555 lr: 2.346e-06\n",
      "[[05/04/2019 02:53:34 AM]] Step 2492: train 0.249273 lr: 2.488e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.03it/s]\n",
      "[[05/04/2019 02:53:52 AM]] Snapshot loss 0.345122\n",
      "[[05/04/2019 02:53:54 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:53:54 AM]] New low\n",
      "\n",
      "[[05/04/2019 02:53:54 AM]] ====================Epoch 8====================\n",
      "[[05/04/2019 02:54:53 AM]] Step 2670: train 0.206948 lr: 2.630e-06\n",
      "[[05/04/2019 02:55:53 AM]] Step 2848: train 0.186458 lr: 2.772e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 02:56:11 AM]] Snapshot loss 0.361899\n",
      "[[05/04/2019 02:56:11 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:56:11 AM]] ====================Epoch 9====================\n",
      "[[05/04/2019 02:57:10 AM]] Step 3026: train 0.162499 lr: 2.914e-06\n",
      "[[05/04/2019 02:58:10 AM]] Step 3204: train 0.143526 lr: 3.056e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 02:58:28 AM]] Snapshot loss 0.391065\n",
      "[[05/04/2019 02:58:28 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 02:58:28 AM]] ====================Epoch 10====================\n",
      "[[05/04/2019 02:59:27 AM]] Step 3382: train 0.114683 lr: 3.198e-06\n",
      "[[05/04/2019 03:00:27 AM]] Step 3560: train 0.101654 lr: 3.340e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 03:00:46 AM]] Snapshot loss 0.435625\n",
      "[[05/04/2019 03:00:46 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:00:46 AM]] ====================Epoch 11====================\n",
      "[[05/04/2019 03:01:42 AM]] Step 3738: train 0.079788 lr: 3.483e-06\n",
      "[[05/04/2019 03:02:43 AM]] Step 3916: train 0.065227 lr: 3.625e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 03:03:02 AM]] Snapshot loss 0.468531\n",
      "[[05/04/2019 03:03:02 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:03:03 AM]] ====================Epoch 12====================\n",
      "[[05/04/2019 03:03:59 AM]] Step 4094: train 0.049215 lr: 3.767e-06\n",
      "[[05/04/2019 03:05:00 AM]] Step 4272: train 0.040710 lr: 3.909e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 03:05:20 AM]] Snapshot loss 0.522843\n",
      "[[05/04/2019 03:05:20 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:05:20 AM]] ====================Epoch 13====================\n",
      "[[05/04/2019 03:06:17 AM]] Step 4450: train 0.038103 lr: 4.051e-06\n",
      "[[05/04/2019 03:07:17 AM]] Step 4628: train 0.036614 lr: 4.193e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 03:07:37 AM]] Snapshot loss 0.565360\n",
      "[[05/04/2019 03:07:37 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:07:37 AM]] ====================Epoch 14====================\n",
      "[[05/04/2019 03:08:32 AM]] Step 4806: train 0.035089 lr: 4.335e-06\n",
      "[[05/04/2019 03:09:34 AM]] Step 4984: train 0.031172 lr: 4.477e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 03:09:54 AM]] Snapshot loss 0.605692\n",
      "[[05/04/2019 03:09:54 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:09:55 AM]] ====================Epoch 15====================\n",
      "[[05/04/2019 03:10:52 AM]] Step 5162: train 0.023928 lr: 4.619e-06\n",
      "[[05/04/2019 03:11:52 AM]] Step 5340: train 0.022510 lr: 4.761e-06\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.02it/s]\n",
      "[[05/04/2019 03:12:12 AM]] Snapshot loss 0.594256\n",
      "[[05/04/2019 03:12:12 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 28/28 [00:15<00:00,  2.01it/s]\n",
      "[[05/04/2019 03:12:29 AM]] Confirm val loss: 0.3451\n",
      "100%|██████████| 387/387 [02:20<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Fold 5\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/04/2019 03:14:57 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz from cache at /tmp/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233\n",
      "05/04/2019 03:14:57 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /tmp/.pytorch_pretrained_bert/7fb0534b83c42daee7d3ddb0ebaa81387925b71665d6ea195c5447f1077454cd.eea60d9ebb03c75bb36302aa9d241d3b7a04bba39c360cf035e8bf8140816233 to temp dir /tmp/tmpuomuz6fl\n",
      "05/04/2019 03:15:12 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "[[05/04/2019 03:15:22 AM]] SEED: 9293\n",
      "[[05/04/2019 03:15:22 AM]] # of paramters: 338,302,980\n",
      "[[05/04/2019 03:15:22 AM]] # of trainable paramters: 155,878,404\n",
      "[[05/04/2019 03:15:22 AM]] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 1e-05\n",
      "    lr: 5.000000000000001e-07\n",
      "    weight_decay: 0\n",
      ")\n",
      "[[05/04/2019 03:15:22 AM]] Batches per epoch: 357\n",
      "[[05/04/2019 03:15:22 AM]] ====================Epoch 1====================\n",
      "[[05/04/2019 03:16:24 AM]] Step 178: train 1.014269 lr: 6.405e-07\n",
      "[[05/04/2019 03:17:24 AM]] Step 356: train 0.943361 lr: 7.826e-07\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:17:38 AM]] Snapshot loss 0.906227\n",
      "[[05/04/2019 03:17:40 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:17:40 AM]] New low\n",
      "\n",
      "[[05/04/2019 03:17:40 AM]] ====================Epoch 2====================\n",
      "[[05/04/2019 03:18:41 AM]] Step 534: train 0.880068 lr: 9.247e-07\n",
      "[[05/04/2019 03:19:42 AM]] Step 712: train 0.814551 lr: 1.067e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:19:57 AM]] Snapshot loss 0.791290\n",
      "[[05/04/2019 03:19:59 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:19:59 AM]] New low\n",
      "\n",
      "[[05/04/2019 03:19:59 AM]] ====================Epoch 3====================\n",
      "[[05/04/2019 03:20:59 AM]] Step 890: train 0.731601 lr: 1.209e-06\n",
      "[[05/04/2019 03:22:00 AM]] Step 1068: train 0.672961 lr: 1.351e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.92it/s]\n",
      "[[05/04/2019 03:22:15 AM]] Snapshot loss 0.665405\n",
      "[[05/04/2019 03:22:17 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:22:17 AM]] New low\n",
      "\n",
      "[[05/04/2019 03:22:17 AM]] ====================Epoch 4====================\n",
      "[[05/04/2019 03:23:20 AM]] Step 1246: train 0.592757 lr: 1.493e-06\n",
      "[[05/04/2019 03:24:19 AM]] Step 1424: train 0.507139 lr: 1.635e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:24:34 AM]] Snapshot loss 0.485852\n",
      "[[05/04/2019 03:24:37 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:24:37 AM]] New low\n",
      "\n",
      "[[05/04/2019 03:24:37 AM]] ====================Epoch 5====================\n",
      "[[05/04/2019 03:25:35 AM]] Step 1602: train 0.422623 lr: 1.777e-06\n",
      "[[05/04/2019 03:26:37 AM]] Step 1780: train 0.366622 lr: 1.919e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:26:53 AM]] Snapshot loss 0.402882\n",
      "[[05/04/2019 03:26:55 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:26:55 AM]] New low\n",
      "\n",
      "[[05/04/2019 03:26:55 AM]] ====================Epoch 6====================\n",
      "[[05/04/2019 03:27:54 AM]] Step 1958: train 0.307728 lr: 2.062e-06\n",
      "[[05/04/2019 03:28:55 AM]] Step 2136: train 0.277120 lr: 2.204e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:29:11 AM]] Snapshot loss 0.392647\n",
      "[[05/04/2019 03:29:13 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:29:13 AM]] New low\n",
      "\n",
      "[[05/04/2019 03:29:13 AM]] ====================Epoch 7====================\n",
      "[[05/04/2019 03:30:14 AM]] Step 2314: train 0.259835 lr: 2.346e-06\n",
      "[[05/04/2019 03:31:14 AM]] Step 2492: train 0.215375 lr: 2.488e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:31:30 AM]] Snapshot loss 0.403286\n",
      "[[05/04/2019 03:31:30 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:31:30 AM]] ====================Epoch 8====================\n",
      "[[05/04/2019 03:32:28 AM]] Step 2670: train 0.190133 lr: 2.630e-06\n",
      "[[05/04/2019 03:33:30 AM]] Step 2848: train 0.173481 lr: 2.772e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:33:47 AM]] Snapshot loss 0.431945\n",
      "[[05/04/2019 03:33:47 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:33:47 AM]] ====================Epoch 9====================\n",
      "[[05/04/2019 03:34:44 AM]] Step 3026: train 0.142156 lr: 2.914e-06\n",
      "[[05/04/2019 03:35:46 AM]] Step 3204: train 0.124560 lr: 3.056e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.92it/s]\n",
      "[[05/04/2019 03:36:03 AM]] Snapshot loss 0.475757\n",
      "[[05/04/2019 03:36:03 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:36:03 AM]] ====================Epoch 10====================\n",
      "[[05/04/2019 03:37:02 AM]] Step 3382: train 0.092553 lr: 3.198e-06\n",
      "[[05/04/2019 03:38:02 AM]] Step 3560: train 0.090419 lr: 3.340e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:38:19 AM]] Snapshot loss 0.516697\n",
      "[[05/04/2019 03:38:19 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:38:19 AM]] ====================Epoch 11====================\n",
      "[[05/04/2019 03:39:16 AM]] Step 3738: train 0.066587 lr: 3.483e-06\n",
      "[[05/04/2019 03:40:18 AM]] Step 3916: train 0.049006 lr: 3.625e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:40:36 AM]] Snapshot loss 0.578996\n",
      "[[05/04/2019 03:40:36 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:40:36 AM]] ====================Epoch 12====================\n",
      "[[05/04/2019 03:41:33 AM]] Step 4094: train 0.040934 lr: 3.767e-06\n",
      "[[05/04/2019 03:42:34 AM]] Step 4272: train 0.037328 lr: 3.909e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:42:52 AM]] Snapshot loss 0.662657\n",
      "[[05/04/2019 03:42:52 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:42:52 AM]] ====================Epoch 13====================\n",
      "[[05/04/2019 03:43:49 AM]] Step 4450: train 0.028369 lr: 4.051e-06\n",
      "[[05/04/2019 03:44:50 AM]] Step 4628: train 0.018040 lr: 4.193e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:45:08 AM]] Snapshot loss 0.693298\n",
      "[[05/04/2019 03:45:08 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:45:08 AM]] ====================Epoch 14====================\n",
      "[[05/04/2019 03:46:04 AM]] Step 4806: train 0.013962 lr: 4.335e-06\n",
      "[[05/04/2019 03:47:05 AM]] Step 4984: train 0.023754 lr: 4.477e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:47:24 AM]] Snapshot loss 0.648017\n",
      "[[05/04/2019 03:47:24 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "[[05/04/2019 03:47:24 AM]] ====================Epoch 15====================\n",
      "[[05/04/2019 03:48:20 AM]] Step 5162: train 0.022386 lr: 4.619e-06\n",
      "[[05/04/2019 03:49:21 AM]] Step 5340: train 0.017952 lr: 4.761e-06\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.91it/s]\n",
      "[[05/04/2019 03:49:40 AM]] Snapshot loss 0.699578\n",
      "[[05/04/2019 03:49:40 AM]] Saving checkpoint cache/model_cache/best.pth...\n",
      "100%|██████████| 28/28 [00:14<00:00,  1.90it/s]\n",
      "[[05/04/2019 03:49:56 AM]] Confirm val loss: 0.3926\n",
      "100%|██████████| 387/387 [02:20<00:00,  2.76it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_span_extractor = BERTSpanExtractor(dev_df, val_df, test_df_prod, train_batch_size=10, n_epochs=15, bert_model='bert-large-cased') #, learning_rate=1e-3, n_epochs=5)\n",
    "bert_span_val_preds, bert_span_test_preds, bert_span_val_losses = bert_span_extractor.run_k_fold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_test_probas = np.mean(bert_span_test_preds, axis=0)\n",
    "submission_df = pd.DataFrame([test_df_prod.ID, span_test_probas[:,0], span_test_probas[:,1], span_test_probas[:,2]], index=['ID', 'A', 'B', 'NEITHER']).transpose()\n",
    "\n",
    "submission_df.to_csv('stage2_span_only_cased.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
